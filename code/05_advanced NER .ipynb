{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6a64a1-819a-4ca1-afaa-2063d47aa2ee",
   "metadata": {},
   "source": [
    "# Conditional Random Fields\n",
    "\n",
    "Conditional Random Fields (CRFs) are a type of statistical modeling method well-suited for structured prediction tasks, which involve predicting sequences of labels for sequences of input items. They are particularly popular in natural language processing (NLP) for tasks such as named entity recognition, part-of-speech tagging, and many others that involve making predictions that depend on the context of other elements in the sequence.\n",
    "\n",
    "Background and Basics CRFs belong to the family of probabilistic graphical models and are a form of undirected graphical model (Markov random field). Unlike other simpler models such as the Hidden Markov Model (HMM) that make strong independence assumptions, CRFs model the conditional probability of a sequence of labels given a sequence of input tokens. This approach allows CRFs to take into account the interdependencies among the output labels and utilize a broad context of observable features within the input sequence.\n",
    "\n",
    "Mathematical Formulation Formally, a CRF defines a conditional probability distribution  $P(Yâˆ£X)$ over label sequences $Y$ given a particular input sequence ð‘‹ X, rather than modeling the joint distribution $P(X,Y)$. For a sequence $X=(x 1â€‹,x 2â€‹,...,x nâ€‹)$ and corresponding labels $Y=(y 1â€‹,y 2â€‹,...,y nâ€‹)$, the probability of a label sequence given the input sequence is defined as:\n",
    "\n",
    "$$ ð‘ƒ ( ð‘Œ âˆ£ ð‘‹ ) = {1 \\over ð‘( ð‘‹ ) }exp â¡ ( âˆ‘ ð‘– = 1 ð‘› âˆ‘ ð‘˜ \\lambda ð‘˜ ð‘“ ð‘˜ ( ð‘¦ ð‘– âˆ’ 1 , ð‘¦ ð‘– , ð‘‹ , ð‘– ) ) P(Yâˆ£X)= Z(X) 1â€‹exp( i=1 âˆ‘ n$$\n",
    "\n",
    "k âˆ‘â€‹Î» kâ€‹f kâ€‹(y iâˆ’1â€‹,y iâ€‹,X,i)) Where:\n",
    "\n",
    "ð‘“ ð‘˜ f kâ€‹are feature functions that depend on the input sequence ð‘‹ X, the position ð‘– i in the sequence, and the labels at positions ð‘– i and ð‘– âˆ’ 1 iâˆ’1. ðœ† ð‘˜ Î» kâ€‹are weights learned from training data. ð‘ ( ð‘‹ ) Z(X) is a normalization factor that ensures the probabilities sum to 1, computed as the sum of the exponentiated scores of all possible label sequences given ð‘‹ X. Advantages of CRFs Contextual Dependency: CRFs can model complex dependencies between labels, which is particularly useful in tasks where context significantly influences the output, such as in syntactic parsing or where labels are not independently set but are influenced by adjacent labels.\n",
    "\n",
    "Feature Flexibility: Unlike models like HMMs that typically rely on fixed state-transition probabilities, CRFs can incorporate an arbitrary number of overlapping, interdependent features. This allows for inclusion of diverse information such as word tokens, part-of-speech tags, and other lexical features.\n",
    "\n",
    "No Independence Assumptions: CRFs do not assume independence between the input features given the output labels, allowing them to capture more information about the input.\n",
    "\n",
    "Limitations of CRFs Computational Complexity: Training CRFs, particularly over large datasets or with many features, can be computationally intensive due to the need to compute normalization factors across potentially large label spaces.\n",
    "\n",
    "Feature Engineering: Although CRFs are powerful, they often require careful feature engineering to achieve optimal performance. This process can be labor-intensive and requires domain expertise.\n",
    "\n",
    "Scalability Issues: CRFs can struggle to scale to very large datasets or very long sequences due to the computational costs associated with the inference and learning processes.\n",
    "\n",
    "Common Applications Named Entity Recognition (NER): Identifying and classifying named entities in text into predefined categories such as names of persons, organizations, locations. Part-of-Speech Tagging: Assigning part of speech to each word in a sentence, such as noun, verb, adjective, etc. Segmentation and Labeling: Any task that requires segmenting a sequence and labeling its segments, such as tokenization or sentence breaking. CRFs have been foundational in advancing the field of NLP and remain relevant even as newer methods like deep learning-based approaches have begun to dominate the field. They are still used either standalone or as part of hybrid systems that combine traditional machine learning techniques with neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "50761bf3-ea3a-4459-a2b2-e4fdb25bcd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.73      0.71      0.72       446\n",
      "      B-MISC       0.78      0.48      0.59       197\n",
      "       B-ORG       0.68      0.35      0.46       523\n",
      "       B-PER       0.73      0.74      0.74       672\n",
      "       I-LOC       0.89      0.46      0.60        68\n",
      "      I-MISC       0.64      0.65      0.65        83\n",
      "       I-ORG       0.60      0.40      0.48       223\n",
      "       I-PER       0.74      0.94      0.83       493\n",
      "           O       0.95      0.98      0.97      9440\n",
      "\n",
      "    accuracy                           0.91     12145\n",
      "   macro avg       0.75      0.63      0.67     12145\n",
      "weighted avg       0.90      0.91      0.90     12145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, flatten\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "sample_size = 1000\n",
    "# Load CoNLL-2003 dataset\n",
    "dataset = datasets.load_dataset('conll2003', trust_remote_code=True)\n",
    "\n",
    "train_tokens = dataset[\"train\"][\"tokens\"][:sample_size]\n",
    "train_tags = dataset[\"train\"][\"ner_tags\"][:sample_size]\n",
    "\n",
    "test_tokens = dataset[\"test\"][\"tokens\"][:sample_size]\n",
    "test_tags = dataset[\"test\"][\"ner_tags\"][:sample_size]\n",
    "\n",
    "#converting the integer values of ner_tags to their coressponding ner_tag values as the dictionary given in data description\n",
    "def int_ner(data):\n",
    "    d = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "\n",
    "    d1 = {v:k for k, v in d.items()}\n",
    "    i=0\n",
    "    for elements in data:\n",
    "        for j in range(len(elements)):\n",
    "            data[i][j] = d1[elements[j]]\n",
    "        i+=1\n",
    "    return data\n",
    "\n",
    "train_tags = int_ner(train_tags)\n",
    "test_tabs = int_ner(test_tags)\n",
    "# Feature extraction functions\n",
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of Sentence\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of Sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return sent\n",
    "\n",
    "# Prepare data\n",
    "X_train = [sent2features(s) for s in train_tokens]\n",
    "y_train = train_tags\n",
    "X_test = [sent2features(s) for s in test_tokens]\n",
    "y_test = test_tags\n",
    "\n",
    "# Train CRF\n",
    "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True)\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(flatten(y_test), flatten(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac723c-e705-42c6-ae94-80b42433560e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
